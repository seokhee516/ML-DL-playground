{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "_객체 탐지 (Object Detection) - YOLO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seokhee516/ML-DL-playground/blob/main/_%EA%B0%9D%EC%B2%B4_%ED%83%90%EC%A7%80_(Object_Detection)_YOLO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL799-9jT1W7"
      },
      "source": [
        "- 참조 : https://github.com/wikibook/dl-vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE7-X_QfSm0G"
      },
      "source": [
        "# 객체 탐지 (Object Detection)\n",
        "\n",
        "- 한 이미지에서 객체와 그 경계 상자(bounding box)를 탐지\n",
        "\n",
        "- 객체 탐지 알고리즘은 일반적으로 이미지를 입력으로 받고, 경계 상자와 객체 클래스 리스트를 출력\n",
        "\n",
        "- 경계 상자에 대해 그에 대응하는 예측 클래스와 클래스의 신뢰도(confidence)를 출력\n",
        "\n",
        "## Applications\n",
        "\n",
        "- 자율 주행 자동차에서 다른 자동차와 보행자를 찾을 때\n",
        "\n",
        "- 의료 분야에서 방사선 사진을 사용해 종양이나 위험한 조직을 찾을 때\n",
        "\n",
        "- 제조업에서 조립 로봇이 제품을 조립하거나 수리할 때\n",
        "\n",
        "- 보안 산업에서 위협을 탐지하거나 사람을 셀 때"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LGNVEwnfNjn"
      },
      "source": [
        "## 용어 설명"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZnxALKYfP8s"
      },
      "source": [
        "### Boudning Box\n",
        "\n",
        "- 이미지에서 하나의 객체 전체를 포함하는 가장 작은 직사각형\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/850/1*KL6r494Eyfh3iYEXQA2tzg.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://medium.com/anolytics/how-bounding-box-annotation-helps-object-detection-in-machine-learning-use-cases-431d93e7b25b</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx5ViAXvwcWS"
      },
      "source": [
        "### IOU(Intersection Over Union)\n",
        "- 실측값(Ground Truth)과 모델이 예측한 값이 얼마나 겹치는지를 나타내는 지표\n",
        "\n",
        "  <img src=\"https://pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png\" width=\"300\">\n",
        "\n",
        "- IOU가 높을수록 잘 예측한 모델\n",
        "\n",
        "  <img src=\"https://pyimagesearch.com/wp-content/uploads/2016/09/iou_examples.png\" width=\"400\">\n",
        "\n",
        "<br>\n",
        "\n",
        "- 예시\n",
        "  <img src=\"https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_stop_sign.jpg\">\n",
        "\n",
        "  <sub>[이미지 출처] https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/</sub>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHVxK0LXqd74"
      },
      "source": [
        "def compute_iou(pred_box, gt_box):\n",
        "  x1 = np.maximum(pred_box[0], gt_box[0])\n",
        "  y1 = np.maximum(pred_box[1], gt_box[1])\n",
        "  x2 = np.maximum(pred_box[2], gt_box[2])\n",
        "  y2 = np.maximum(pred_box[3], gt_box[3])\n",
        "\n",
        "  intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n",
        "\n",
        "  pred_box_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
        "  gt_box_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n",
        "  union = pred_box_area + gt_box_area - intersection_area\n",
        "\n",
        "  iou = intersection / union\n",
        "\n",
        "  return iou"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKL2Ae7SoYDs"
      },
      "source": [
        "### NMS(Non-Maximum Suppression, 비최댓값 억제)\n",
        "\n",
        "- 확률이 가장 높은 상자와 겹치는 상자들을 제거하는 과정\n",
        "\n",
        "- 최댓값을 갖지 않는 상자들을 제거\n",
        "\n",
        "- 과정\n",
        "\n",
        "  1. 확률 기준으로 모든 상자를 정렬하고 먼저 가장 확률이 높은 상자를 취함\n",
        "\n",
        "  2. 각 상자에 대해 다른 모든 상자와의 IOU를 계산\n",
        "\n",
        "  3. 특정 임곗값을 넘는 상자는 제거\n",
        "\n",
        "  <img src=\"https://pyimagesearch.com/wp-content/uploads/2014/10/nms_fast_03.jpg\">\n",
        "\n",
        "  <sub>[이미지 출처] https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/</sub>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTh_WYA0tUBz"
      },
      "source": [
        "import numpy as np\n",
        "def non_max_surppression_fast(boxes, overlap_thresh):\n",
        "  if len(boxes) == 0:\n",
        "    return []\n",
        "  \n",
        "  if boxes.dtype.kind == 'i':\n",
        "    boxes = boxes.astype('float')\n",
        "  \n",
        "  pick = []\n",
        "  x1 = boxes[:, 0]\n",
        "  y1 = boxes[:, 1]\n",
        "  x2 = boxes[:, 2]\n",
        "  y2 = boxes[:, 3]\n",
        "\n",
        "  area = (x2 - x1 +1) * (y2 - y1 + 1)\n",
        "  idxs = np.argsort(y2)\n",
        "  \n",
        "  while len(idxs) > 0:\n",
        "    last = len(idxs) - 1\n",
        "    i = idxs[last]\n",
        "    pick.append(i)\n",
        "\n",
        "    xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
        "    yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
        "    xx2 = np.maximum(x2[i], x2[idxs[:last]])\n",
        "    yy2 = np.maximum(y2[i], y2[idxs[:last]])\n",
        "\n",
        "    w = np.maximum(0, xx2 - xx1 + 1)\n",
        "    h = np.maximum(0, yy2 - yy1 + 1)\n",
        "\n",
        "    overlap = (w * h) /area[idxs[:last]]\n",
        "\n",
        "    idxs = np.delete(idxs, np.concatenate(([last], np.where(overlap > overlap_thresh)[0])))\n",
        "  return boxes[pick].astype('int')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSLe74EnU6zQ"
      },
      "source": [
        "## 모델 성능 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zMP3ecSU1pa"
      },
      "source": [
        "### 정밀도와 재현율\n",
        "\n",
        "- 일반적으로 객체 탐지 모델 평가에 사용되지는 않지만, 다른 지표를 계산하는 기본 지표 역할을 함\n",
        "\n",
        "  - `TP` \n",
        "\n",
        "    - True Positives\n",
        "\n",
        "    - 예측이 동일 클래스의 실제 상자와 일치하는지 측정\n",
        "\n",
        "  - `FP`\n",
        "\n",
        "    - False Positives\n",
        "\n",
        "    - 예측이 실제 상자와 일치하지 않는지 측정\n",
        "\n",
        "  - `FN`\n",
        "\n",
        "    - False Negatives\n",
        "\n",
        "    - 실제 분류값이 그와 일치하는 예측을 갖지 못하는지 측정\n",
        "\n",
        "<br>\n",
        "\n",
        "## $\\qquad precision = \\frac{TP}{TP \\ + \\ FP}$\n",
        "## $\\qquad recall = \\frac{TP}{TP \\ + \\ FN}$\n",
        "\n",
        "  - 모델이 안정적이지 않은 특징을 기반으로 객체 존재를 예측하면 거짓긍정(FP)이 많아져서 정밀도가 낮아짐\n",
        "\n",
        "  - 모델이 너무 엄격해서 정확한 조건을 만족할 때만 객체가 탐지된 것으로 간주하면 거짓부정(FN)이 많아져서 재현율이 낮아짐\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTDOrbKYkSyh"
      },
      "source": [
        "### 정밀도-재현율 곡선(precision-recall curve) \n",
        "\n",
        "- 신뢰도 임곗값마다 모델의 정밀도와 재현율을 시각화\n",
        "\n",
        "- 모든 bounding box와 함께 모델이 예측의 정확성을 얼마나 확실하는지 0 ~ 1사이의 숫자로 나타내는 신뢰도를 출력\n",
        "\n",
        "- 임계값 T에 따라 정밀도와 재현율이 달라짐\n",
        "\n",
        "  - 임곗값 T 이하의 예측은 제거함\n",
        "\n",
        "  - T가 1에 가까우면 정밀도는 높지만 재현율은 낮음  \n",
        "    놓치는 객체가 많아져서 재현율이 낮아짐. 즉, 신뢰도가 높은 예측만 유지하기때문에 정밀도는 높아짐\n",
        "\n",
        "  - T가 0에 가까우면 정밀도는 낮지만 재현율은 높음  \n",
        "    대부분의 예측을 유지하기때문에 재현율은 높아지고, 거짓긍정(FP)이 많아져서 정밀도가 낮아짐\n",
        "\n",
        "- 예를 들어, 모델이 보행자를 탐지하고 있으면 특별한 이유없이 차를 세우더라도 어떤 보행자도 놓치지 않도록 재현율을 높여야 함\n",
        "  모델이 투자 기회를 탐지하고 있다면 일부 기회를 놓치게 되더라도 잘못된 기회에 돈을 거는 일을 피하기 위해 정밀도를 높여야 함\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Davide_Chicco/publication/321672019/figure/fig1/AS:614279602511886@1523467078452/a-Example-of-Precision-Recall-curve-with-the-precision-score-on-the-y-axis-and-the.png\">\n",
        "\n",
        "<sub>[이미지 출처] https://www.researchgate.net/figure/a-Example-of-Precision-Recall-curve-with-the-precision-score-on-the-y-axis-and-the_fig1_321672019</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TNopqHBmh2T"
      },
      "source": [
        "### AP (Average Precision, 평균 정밀도) 와 mAP(mean Average Precision)\n",
        "\n",
        "- 곡선의 아래 영역에 해당\n",
        "\n",
        "- 항상 1x1 정사각형으로 구성되어 있음  \n",
        "  즉, 항상 0 ~ 1 사이의 값을 가짐\n",
        "\n",
        "- 단일 클래스에 대한 모델 성능 정보를 제공\n",
        "\n",
        "- 전역 점수를 얻기위해서 mAP를 사용\n",
        "\n",
        "- 예를 들어, 데이터셋이 10개의 클래스로 구성된다면 각 클래스에 대한 AP를 계산하고, 그 숫자들의 평균을 다시 구함\n",
        "\n",
        "- (참고)\n",
        "\n",
        "  - 최소 2개 이상의 객체를 탐지하는 대회인 PASCAL Visual Object Classes와 Common Objects in Context(COCO)에서 mAP가 사용됨\n",
        "\n",
        "  - COCO 데이터셋이 더 많은 클래스를 포함하고 있기 때문에 보통 Pascal VOC보다 점수가 더 낮게 나옴\n",
        "\n",
        "  - 예시\n",
        "\n",
        "    <img src=\"https://www.researchgate.net/profile/Bong_Nam_Kang/publication/328939155/figure/tbl2/AS:692891936649218@1542209719916/Evaluation-on-PASCAL-VOC-2007-and-MS-COCO-test-dev.png\">\n",
        "\n",
        "    <sub>[이미지 출처] https://www.researchgate.net/figure/Evaluation-on-PASCAL-VOC-2007-and-MS-COCO-test-dev_tbl2_328939155</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN1jyyGEkTvx"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eprofi5rbUuA"
      },
      "source": [
        "### VOC\n",
        "\n",
        "- 2005년부터 2012년까지 진행\n",
        "\n",
        "- Object Detection 기술의 benchmark로 간주\n",
        "\n",
        "- 데이터셋에는 20개의 클래스가 존재\n",
        "\n",
        "      background\n",
        "      aeroplane\n",
        "      bicycle\n",
        "      bird\n",
        "      boat\n",
        "      bottle\n",
        "      bus\n",
        "      car\n",
        "      cat\n",
        "      chair\n",
        "      cow\n",
        "      diningtable\n",
        "      dog\n",
        "      horse\n",
        "      motorbike\n",
        "      person\n",
        "      pottedplant\n",
        "      sheep\n",
        "      sofa\n",
        "      train\n",
        "      tvmonitor\n",
        "\n",
        "- 훈련 및 검증 데이터 : 11,530개\n",
        "\n",
        "- ROI에 대한 27,450개의 Annotation이 존재\n",
        "\n",
        "- 이미지당 2.4개의 객체 존재\n",
        "\n",
        "  <img src=\"https://paperswithcode.github.io/sotabench-eval/img/pascalvoc2012.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://paperswithcode.github.io/sotabench-eval/pascalvoc/</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL1IEr71bWE_"
      },
      "source": [
        "### COCO Dataset\n",
        "\n",
        "- Common Objects in Context\n",
        "\n",
        "- 200,000개의 이미지 \n",
        "\n",
        "- 80개의 카테고리에 500,000개 이상의 객체 Annotation이 존재\n",
        "      person\n",
        "      bicycle\n",
        "      car\n",
        "      motorbike\n",
        "      aeroplane\n",
        "      bus\n",
        "      train\n",
        "      truck\n",
        "      boat\n",
        "      traffic light\n",
        "      fire hydrant\n",
        "      stop sign\n",
        "      parking meter\n",
        "      bench\n",
        "      bird\n",
        "      cat\n",
        "      dog\n",
        "      horse\n",
        "      sheep\n",
        "      cow\n",
        "      elephant\n",
        "      bear\n",
        "      zebra\n",
        "      giraffe\n",
        "      backpack\n",
        "      umbrella\n",
        "      handbag\n",
        "      tie\n",
        "      suitcase\n",
        "      frisbee\n",
        "      skis\n",
        "      snowboard\n",
        "      sports ball\n",
        "      kite\n",
        "      baseball bat\n",
        "      baseball glove\n",
        "      skateboard\n",
        "      surfboard\n",
        "      tennis racket\n",
        "      bottle\n",
        "      wine glass\n",
        "      cup\n",
        "      fork\n",
        "      knife\n",
        "      spoon\n",
        "      bowl\n",
        "      banana\n",
        "      apple\n",
        "      sandwich\n",
        "      orange\n",
        "      broccoli\n",
        "      carrot\n",
        "      hot dog\n",
        "      pizza\n",
        "      donut\n",
        "      cake\n",
        "      chair\n",
        "      sofa\n",
        "      pottedplant\n",
        "      bed\n",
        "      diningtable\n",
        "      toilet\n",
        "      tvmonitor\n",
        "      laptop\n",
        "      mouse\n",
        "      remote\n",
        "      keyboard\n",
        "      cell phone\n",
        "      microwave\n",
        "      oven\n",
        "      toaster\n",
        "      sink\n",
        "      refrigerator\n",
        "      book\n",
        "      clock\n",
        "      vase\n",
        "      scissors\n",
        "      teddy bear\n",
        "      hair drier\n",
        "      toothbrush\n",
        "- https://cocodataset.org/#home\n",
        "\n",
        "<img src=\"https://cocodataset.org/images/coco-examples.jpg\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMmlbrpjeD6e"
      },
      "source": [
        "# YOLO (You Only Look Once)\n",
        "\n",
        "- 가장 빠른 객체 검출 알고리즘 중 하나\n",
        "\n",
        "- 256x256 사이즈의 이미지\n",
        "\n",
        "- GPU 사용 시, 초당 170프레임(170**FPS**, **frames per second**),  \n",
        "  이는 파이썬, 텐서플로 기반 프레임워크가 아닌 C++로 구현된 코드 기준\n",
        "\n",
        "- 작은 크기의 물체를 탐지하는데는 어려움\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*bSLNlG7crv-p-m4LVYYk3Q.png\" width=\"600\">\n",
        "\n",
        "\n",
        "- https://pjreddie.com/darknet/yolo/\n",
        "\n",
        "- 자세한 내용 참조 : https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txORxnfIAegL"
      },
      "source": [
        "## YOLO Backbone\n",
        "\n",
        "- 백본 모델(backbone model) 기반\n",
        "\n",
        "- 특징 추출기(Feature Extractor)라고도 불림\n",
        "\n",
        "- YOLO는 자체 맞춤 아키텍쳐 사용\n",
        "\n",
        "- 어떤 특징 추출기 아키텍쳐를 사용했는지에 따라 성능 달라짐\n",
        "\n",
        "  <img src=\"https://www.researchgate.net/publication/335865923/figure/fig1/AS:804106595758082@1568725360777/Structure-detail-of-YOLOv3It-uses-Darknet-53-as-the-backbone-network-and-uses-three.jpg\">\n",
        "\n",
        "  <sub>[이미지 출처] https://www.researchgate.net/figure/Structure-detail-of-YOLOv3It-uses-Darknet-53-as-the-backbone-network-and-uses-three_fig1_335865923</sub>\n",
        "\n",
        "- 마지막 계층은 크기가 $w \\times h \\times D$인 특징 볼륨 출력\n",
        "\n",
        "- $w \\times h $는 그리드의 크기이고, $D$는 특징 볼륨 깊이\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qWbjby_DwuF"
      },
      "source": [
        "## YOLO의 계층 출력\n",
        "\n",
        "- 마지막 계층 출력은 $w \\times h \\times M$ 행렬\n",
        "  \n",
        "  - $M = B \\times (C + 5)$\n",
        "\n",
        "    - B : 그리드 셀당 경계 상자 개수\n",
        "\n",
        "    - C : 클래스 개수\n",
        "\n",
        "  - 클래스 개수에 5를 더한 이유는 해당 값 만큼의 숫자를 예측해야함\n",
        "\n",
        "    - $t_x$, $t_y$는 경계상자의 중심 좌표를 계산\n",
        "\n",
        "    - $t_w$, $t_h$는 경계상자의 너비와 높이를 계산\n",
        "\n",
        "    - $c$는 객체가 경계 상자 안에 있다고 확신하는 신뢰도\n",
        "\n",
        "    - $p1, p2, ..., pC$는 경계상자가 클래스 1, 2, ..., C의 객체를 포함할 확률\n",
        "  \n",
        "  <br>\n",
        "\n",
        "  <img src=\"https://www.researchgate.net/profile/Thi_Le3/publication/337705605/figure/fig3/AS:831927326089217@1575358339500/Structure-of-one-output-cell-in-YOLO.ppm\">\n",
        "\n",
        "  <sub>[이미지 출처] https://www.researchgate.net/figure/Structure-of-one-output-cell-in-YOLO_fig3_337705605</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AarR-8M1kkAc"
      },
      "source": [
        "## 앵커 박스(Anchor Box)\n",
        "\n",
        "- YOLOv2에서 도입\n",
        "\n",
        "- 사전 정의된 상자(prior box)\n",
        "\n",
        "- 객체에 가장 근접한 앵커 박스를 맞추고 신경망을 사용해 앵커 박스의 크기를 조정하는 과정때문에 $t_x, t_y, t_w, t_h$이 필요\n",
        "\n",
        "  <img src=\"https://kr.mathworks.com/help/vision/ug/ssd_detection.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://kr.mathworks.com/help/vision/ug/getting-started-with-yolo-v2.html</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdToZZ3xokiM"
      },
      "source": [
        "# YOLOv3 Inference 연습 : tensorflow 2\n",
        "\n",
        "- 코드 참조 : https://github.com/zzh8829/yolov3-tf2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B8C3YpS4n2a"
      },
      "source": [
        "## Clone and install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb3jBxF9x-JR",
        "outputId": "c931921d-2c6f-4299-c41c-bea4d141cdaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! git clone https://github.com/zzh8829/yolov3-tf2"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov3-tf2'...\n",
            "remote: Enumerating objects: 439, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 439 (delta 10), reused 3 (delta 0), pack-reused 412\u001b[K\n",
            "Receiving objects: 100% (439/439), 4.25 MiB | 26.67 MiB/s, done.\n",
            "Resolving deltas: 100% (247/247), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov3-tf2/"
      ],
      "metadata": {
        "id": "JnmT0_OdbDEL",
        "outputId": "c500f58d-3909-4225-9aa3-849f79cc776a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov3-tf2/yolov3-tf2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements-gpu.txt"
      ],
      "metadata": {
        "id": "XwWdIIyUPicM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FONrNLUIx-ML",
        "outputId": "841880e0-dd3d-4ba3-d0f7-fcac20eb647c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoints\t data\t\t  README.md\t\ttrain.py\n",
            "colab_gpu.ipynb  detect.py\t  requirements-gpu.txt\tyolov3_tf2\n",
            "conda-cpu.yml\t detect_video.py  requirements.txt\tyolov3_tf2.egg-info\n",
            "conda-gpu.yml\t docs\t\t  setup.py\n",
            "convert.py\t LICENSE\t  tools\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2QJVUyc4io-"
      },
      "source": [
        "## Check Tensorflow2 version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.1.2"
      ],
      "metadata": {
        "id": "q3eaTQF9P7uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41me-hpBx-fD",
        "outputId": "8ef7ef58-216d-4b33-b40b-8b371927350e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-10-4ae80da9abf6>\", line 2, in <module>\n",
            "    tf.__version__\n",
            "AttributeError: module 'tensorflow' has no attribute '__version__'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 733, in getmodule\n",
            "    if ismodule(module) and hasattr(module, '__file__'):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
            "    module = self._load()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 44, in _load\n",
            "    module = _importlib.import_module(self.__name__)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/__init__.py\", line 46, in <module>\n",
            "    from . _api.v2 import compat\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/__init__.py\", line 39, in <module>\n",
            "    from . import v1\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/__init__.py\", line 32, in <module>\n",
            "    from . import compat\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/compat/__init__.py\", line 39, in <module>\n",
            "    from . import v1\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/compat/v1/__init__.py\", line 29, in <module>\n",
            "    from tensorflow._api.v2.compat.v1 import app\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/__init__.py\", line 39, in <module>\n",
            "    from . import v1\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/__init__.py\", line 32, in <module>\n",
            "    from . import compat\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/compat/__init__.py\", line 39, in <module>\n",
            "    from . import v1\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/compat/v1/__init__.py\", line 35, in <module>\n",
            "    from tensorflow._api.v2.compat.v1 import debugging\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/debugging/__init__.py\", line 10, in <module>\n",
            "    from . import experimental\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/debugging/experimental/__init__.py\", line 10, in <module>\n",
            "    from tensorflow.python.debug.lib.dumping_callback import disable_dump_debug_info\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/lib/dumping_callback.py\", line 29, in <module>\n",
            "    from tensorflow.python.debug.lib import debug_events_writer\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/lib/debug_events_writer.py\", line 24, in <module>\n",
            "    from tensorflow.python import _pywrap_debug_events_writer\n",
            "  File \"<frozen importlib._bootstrap>\", line 1032, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
            "    module = self._load()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 44, in _load\n",
            "    module = _importlib.import_module(self.__name__)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/__init__.py\", line 95, in <module>\n",
            "    from tensorflow.python import keras\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/__init__.py\", line 27, in <module>\n",
            "    from tensorflow.python.keras import models\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/__init__.py\", line 27, in <module>\n",
            "    from tensorflow.python.keras import models\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/models.py\", line 26, in <module>\n",
            "    from tensorflow.python.keras.engine import sequential\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/sequential.py\", line 28, in <module>\n",
            "    from tensorflow.python.keras.engine import training\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 46, in <module>\n",
            "    from tensorflow.python.keras.engine import training_arrays\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 41, in <module>\n",
            "    from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/sparse/__init__.py\", line 229, in <module>\n",
            "    from .base import *\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/sparse/base.py\", line 7, in <module>\n",
            "    from scipy._lib._numpy_compat import broadcast_to\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/_lib/_numpy_compat.py\", line 16, in <module>\n",
            "    _assert_warns = np.testing.assert_warns\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/__init__.py\", line 213, in __getattr__\n",
            "    import numpy.testing as testing\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/testing/__init__.py\", line 18, in <module>\n",
            "    __all__ = _private.utils.__all__ + ['TestCase', 'run_module_suite']\n",
            "NameError: name '_private' is not defined\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z-L1V51ziJE"
      },
      "source": [
        "## Convert Pretrained Darknet Weight\n",
        "\n",
        "- https://pjreddie.com/media/files/yolov3.weights\n",
        "- `yolov3.weights`를 `/yolov3-tf2/data/`로 넣기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq6yz4Mpx--B",
        "outputId": "fc3bfe33-36a4-4b11-b603-ff5903334dbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights -0 data/yolov3.weights\n",
        "!python convert.py"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wget: invalid option -- '0'\n",
            "Usage: wget [OPTION]... [URL]...\n",
            "\n",
            "Try `wget --help' for more options.\n",
            "2022-02-11 06:16:36.486246: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-02-11 06:16:36.486334: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-02-11 06:16:36.486349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2022-02-11 06:16:37.343562: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-02-11 06:16:37.395453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-11 06:16:37.396053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2022-02-11 06:16:37.407669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2022-02-11 06:16:37.584790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2022-02-11 06:16:37.608797: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2022-02-11 06:16:37.633281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2022-02-11 06:16:37.856259: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2022-02-11 06:16:37.877236: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2022-02-11 06:16:38.202582: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2022-02-11 06:16:38.202765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-11 06:16:38.203467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-11 06:16:38.204046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
            "2022-02-11 06:16:38.211865: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-02-11 06:16:38.216755: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2022-02-11 06:16:38.217016: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e9ade3cf40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-02-11 06:16:38.217044: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-02-11 06:16:38.468721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-11 06:16:38.469528: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e9ade3d100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2022-02-11 06:16:38.469559: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2022-02-11 06:16:38.469721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-11 06:16:38.470284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2022-02-11 06:16:38.470350: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2022-02-11 06:16:38.470366: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2022-02-11 06:16:38.470378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2022-02-11 06:16:38.470391: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2022-02-11 06:16:38.470404: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2022-02-11 06:16:38.470416: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2022-02-11 06:16:38.470429: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2022-02-11 06:16:38.470487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-11 06:16:38.471050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-11 06:16:38.471536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
            "2022-02-11 06:16:38.471598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2022-02-11 06:16:38.472844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2022-02-11 06:16:38.472874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \n",
            "2022-02-11 06:16:38.472882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \n",
            "2022-02-11 06:16:38.473070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-11 06:16:38.473662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-11 06:16:38.474352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15224 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "Model: \"yolov3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, None, None,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "yolo_darknet (Model)            ((None, None, None,  40620640    input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "yolo_conv_0 (Model)             (None, None, None, 5 11024384    yolo_darknet[1][2]               \n",
            "__________________________________________________________________________________________________\n",
            "yolo_conv_1 (Model)             (None, None, None, 2 2957312     yolo_conv_0[1][0]                \n",
            "                                                                 yolo_darknet[1][1]               \n",
            "__________________________________________________________________________________________________\n",
            "yolo_conv_2 (Model)             (None, None, None, 1 741376      yolo_conv_1[1][0]                \n",
            "                                                                 yolo_darknet[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "yolo_output_0 (Model)           (None, None, None, 3 4984063     yolo_conv_0[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "yolo_output_1 (Model)           (None, None, None, 3 1312511     yolo_conv_1[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "yolo_output_2 (Model)           (None, None, None, 3 361471      yolo_conv_2[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "yolo_boxes_0 (Lambda)           ((None, None, None,  0           yolo_output_0[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "yolo_boxes_1 (Lambda)           ((None, None, None,  0           yolo_output_1[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "yolo_boxes_2 (Lambda)           ((None, None, None,  0           yolo_output_2[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "yolo_nms (Lambda)               ((1, None, 4), (1, N 0           yolo_boxes_0[0][0]               \n",
            "                                                                 yolo_boxes_0[0][1]               \n",
            "                                                                 yolo_boxes_0[0][2]               \n",
            "                                                                 yolo_boxes_1[0][0]               \n",
            "                                                                 yolo_boxes_1[0][1]               \n",
            "                                                                 yolo_boxes_1[0][2]               \n",
            "                                                                 yolo_boxes_2[0][0]               \n",
            "                                                                 yolo_boxes_2[0][1]               \n",
            "                                                                 yolo_boxes_2[0][2]               \n",
            "==================================================================================================\n",
            "Total params: 62,001,757\n",
            "Trainable params: 61,949,149\n",
            "Non-trainable params: 52,608\n",
            "__________________________________________________________________________________________________\n",
            "I0211 06:16:43.003975 139832740792192 convert.py:24] model created\n",
            "Traceback (most recent call last):\n",
            "  File \"convert.py\", line 39, in <module>\n",
            "    app.run(main)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 312, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 258, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"convert.py\", line 26, in main\n",
            "    load_darknet_weights(yolo, FLAGS.weights, FLAGS.tiny)\n",
            "  File \"/content/yolov3-tf2/yolov3_tf2/utils.py\", line 26, in load_darknet_weights\n",
            "    wf = open(weights_file, 'rb')\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './data/yolov3.weights'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcz5iiyrzY9j"
      },
      "source": [
        "## Initial Detector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow-estimator tensorboard tb-nightly tf-estimator-nightly\n",
        "!pip install tensorflow-estimator==2.1.2"
      ],
      "metadata": {
        "id": "KSgv3dUYbfbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "SM1GsI2NdS5L",
        "outputId": "823f42a9-f2c8-40e7-dc6c-62cd6978b828",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing==1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow) (1.5.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rt1bEf4x-7e"
      },
      "source": [
        "import sys \n",
        "from absl import app, logging, flags\n",
        "from absl.flags import FLAGS\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from yolov3_tf2.models import YoloV3, YoloV3Tiny\n",
        "from yolov3_tf2.dataset import transfrom_images, load_tfrecord_dataset\n",
        "from yolov3_tf2.utils import draw_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flags.DEFINE_string('classes', './data/coco.names', 'path to classes file')\n",
        "flags.DEFINE_string('weights', './checkpoints/yolov3.tf','path to weights file')\n",
        "flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\n",
        "flags.DEFINE_integer('size', 416, 'resize images to')\n",
        "flags.DEFINE_string('image', './data/girl.png', 'path to input image')\n",
        "flags.DEFINE_string('tfrecord', None, 'tfrecord instead of image')\n",
        "flags.DEFINE_string('output', './output.jpg', 'path to output image')\n",
        "flags.DEFINE_integer('num_classes', 80, 'number of classes in the model')\n",
        "\n",
        "app._run_init(['yolov3'], app.parse_flags_with_usage)\n",
        "\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "metadata": {
        "id": "bJ8HYeEeYFJ6",
        "outputId": "02dc3d9b-6146-4a2b-ad2e-4da51c1774cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/     \u001b[01;34mdata\u001b[0m/            README.md             train.py\n",
            "colab_gpu.ipynb  detect.py        requirements-gpu.txt  \u001b[01;34myolov3_tf2\u001b[0m/\n",
            "conda-cpu.yml    detect_video.py  requirements.txt      \u001b[01;34myolov3-tf2\u001b[0m/\n",
            "conda-gpu.yml    \u001b[01;34mdocs\u001b[0m/            setup.py              \u001b[01;34myolov3_tf2.egg-info\u001b[0m/\n",
            "convert.py       LICENSE          \u001b[01;34mtools\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qBaqsEG41l_"
      },
      "source": [
        "## Detect Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbPOCGuxx-2t"
      },
      "source": [
        "FLAGS.image = 'data/meme.jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsa4Dklsx-0_"
      },
      "source": [
        "if FLAGS.tiny:\n",
        "    yolo = YoloV3Tiny(classes=FLAGS.num_classes)\n",
        "else:\n",
        "    yolo = YoloV3(classes=FLAGS.num_classes)\n",
        "      \n",
        "yolo.load_weights(FLAGS.weights).expect_partial()\n",
        "logging.info('weights loaded')\n",
        "\n",
        "class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n",
        "logging.info('classes loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkvDwZsux-yN"
      },
      "source": [
        "img_raw = tf.image.decode_image(open(FLAGS.image, 'rb').read(), channels=3)\n",
        "img = tf.expand_dims(img_raw, 0)\n",
        "img = transform_images(img, FLAGS.size)\n",
        "\n",
        "t1 = time.time()\n",
        "boxes, scores, classes, nums = yolo(img)\n",
        "t2 = time.time()\n",
        "logging.info('time: {}'.format(t2 - t1))\n",
        "\n",
        "logging.info('detections:')\n",
        "for i in range(nums[0]):\n",
        "    logging.info('\\t{}, {}, {}'.format(class_names[int(classes[0][i])],\n",
        "                                        np.array(scores[0][i]),\n",
        "                                        np.array(boxes[0][i])))\n",
        "\n",
        "img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)\n",
        "img = draw_outputs(img, (boxes, scores, classes, nums), class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfKcTD_Ax-v7"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image(data=bytes(cv2.imencode('.jpg', img)[1]), width=800))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLr782wcsFVT"
      },
      "source": [
        "# YOLOv3 inference 연습 : Pytorch \n",
        "\n",
        "- video inference\n",
        "- 코드 참조 : https://towardsdatascience.com/yolov3-pytorch-on-google-colab-c4a79eeecdea\n",
        "- https://github.com/ultralytics/yolov3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK71PKqk4aTH"
      },
      "source": [
        "## Prepare YOLOv3-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrC5-LmG3F6E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgl_VY_Tq97I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R897GokLq95E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NfJM0tw3TFa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mrlt91dYq915"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0lpUVOdq9zn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbUVelkiszUD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwhHYatUq9xx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlEcyGxh3iOI"
      },
      "source": [
        "## Git clone to get short videos\n",
        "\n",
        "- https://github.com/vindruid/yolov3-in-colab.git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoslGlSkq9vo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRqM_huM3pIS"
      },
      "source": [
        "## Process Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNVhPZ0Jq9s5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC2axiin3nja"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUldkiZA3wQt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EflrkImk3rqF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd4cy_Gd30gu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39Nk92gJ5a4J"
      },
      "source": [
        "- Custom Data Train 참고\n",
        "\n",
        "  - https://github.com/AntonMu/TrainYourOwnYOLO"
      ]
    }
  ]
}