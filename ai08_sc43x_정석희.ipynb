{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNTnmwgAOc-H"
      },
      "source": [
        "<img src='https://user-images.githubusercontent.com/6457691/90080969-0f758d00-dd47-11ea-8191-fa12fd2054a7.png' width = '200' align = 'right'>\n",
        "\n",
        "## AIB / SECTION 4 / SC43x \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OInq8enOzzUR"
      },
      "source": [
        "# Major Neural Network Architectures\n",
        "\n",
        "이번 한 주간 CNN, U-Net, Autoencoder, GAN 등 다양한 주요 신경망 구조들에 대해서 배워봤습니다. 오늘은 그 모델들을 복습하는 시간을 가지도록 하겠습니다. 이 SC는 **신경망의 다양한 구조에 대한 이해와 지식**을 평가합니다. **모델을 높은 정확도를 가지도록 학습 시킬 수 있는지를 평가하려는 것이 아닙니다.**\n",
        "\n",
        "아래의 방식들은 복잡한 연산을 요구합니다. 모든 파트의 문제들은 어떤 환경에서라도 (e.g. 로컬 주피터, Google Colab, etc.) 5-10분 내외로 결과값이 나오도록 제작이 됐기 때문에 만일 결과값을 도출하는데 그 이상의 시간이 걸린다면 여러분의 접근 방식을 재점검해보시기 바랍니다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz0LCZd_O4IG"
      },
      "source": [
        "## 1. CNN\n",
        "\n",
        "### 이미지 분류\n",
        "Keras와  [ResNet50v2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2) (pre-trained)을 활용하여 `im_frog` 폴더에 있는 이미지 중 어떤 이미지에 개구리가 있는지 찾는 이미지 분류 모델을 작동시켜 보겠습니다.\n",
        "\n",
        "<img align=\"left\" src=\"https://d3i6fh83elv35t.cloudfront.net/newshour/app/uploads/2017/03/GettyImages-654745934-1024x687.jpg\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ_gNvCvOb6h"
      },
      "source": [
        "## 2.1 Resnet$V_2$을 사용하기 위해서 전처리 함수를 사용하여 이미지를 전처리 하고 이미지들의 사이즈를 재조정하는 함수를 정의해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6stqwDYDOb6h",
        "outputId": "e0c65397-1f83-4b0c-8e72-1185db119a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPtlO4NZzCMT"
      },
      "source": [
        "- **파일 경로와 이름을 불러옵니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "HO-F6bMiz5zp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "frog_dir = \"./drive/MyDrive/ai-sc43x/im_frog\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "oxnjpYNxLKCf"
      },
      "outputs": [],
      "source": [
        "\"\"\"Hint : os 패키지 내에 파일 이름을 불러올 수 있는 메서드를 사용합니다.\"\"\"\n",
        "\n",
        "filenames = os.listdir(frog_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_rIVzpfzCMW"
      },
      "source": [
        "- **필요한 라이브러리 import 후 이미지를 불러와 예측하는 함수를 정의합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kba76qCAzCMW"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "HaQSwBj92oHY"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocessing(base_dir, name, model):\n",
        "    \"\"\"\n",
        "    이미지 1장을 받아 모델로 예측한 뒤\n",
        "    가장 확률이 높은 클래스 번호를 출력하는 함수입니다.\n",
        "    \n",
        "    Hint:\n",
        "        1. \n",
        "        2. array의 값을 직접 나누어 픽셀 값을 정규화합니다.\n",
        "        \n",
        "    Args:\n",
        "        base_dir : 이미지 파일이 있는 경로입니다.\n",
        "        name : 이미지 파일의 이름입니다.\n",
        "        model : 예측에 사용할 모델입니다.\n",
        "    \"\"\"\n",
        "    image_path = base_dir\n",
        "    image = load_img(image_path+'/'+str(name))\n",
        "    input_arr = img_to_array(image) / 255\n",
        "    \n",
        "    input_arr = np.array([input_arr])\n",
        "    predictions = model.predict(input_arr)\n",
        "    predict_class = np.argmax(predictions, axis=1) \n",
        "    print(predict_class[0])\n",
        "\n",
        "    return predict_class[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRMSyu5VOb6h"
      },
      "source": [
        "## 2.2 ResNet50v2 모델을 사용해 이미지 분류(예측)를 진행합니다. 예측 결과는 자유롭게 출력해봅니다.\n",
        "> 참고: `ResNet50v2`는 \"frog\"로 예측하지 않습니다. \"frog\"의 label은 \"bullfrog, treefrog, tailed frog\"입니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaT07ddW3nHz"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, decode_predictions, preprocess_input\n",
        "\n",
        "resnetV2 = ResNet50V2(weights='imagenet', include_top = True, input_shape=(224,224,3))\n",
        "resnetV2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "resnetV2.trainable=False\n",
        "\n",
        "model = Sequential()\n",
        "model.add(resnetV2)\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "p1lM3Yl-J-H6"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peczf_M3zCMY"
      },
      "source": [
        "- **기존에 정의한 함수(`load_and_preprocessing`)를 사용하여 이미지 분류를 진행하여봅니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "68dG0ORN12N1",
        "outputId": "e27edc48-39d9-4c02-cb2e-5a1143720db7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-90f194415dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 출력되는 클래스의 번호를 모두 리스트에 저장하여 predict_class 에 할당합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredict_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_and_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-92-90f194415dee>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 출력되는 클래스의 번호를 모두 리스트에 저장하여 predict_class 에 할당합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredict_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_and_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-0e1f77c5f8c6>\u001b[0m in \u001b[0;36mload_and_preprocessing\u001b[0;34m(base_dir, name, model)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0minput_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mpredict_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(None, 4928, 3285, 3)\n"
          ]
        }
      ],
      "source": [
        "# 출력되는 클래스의 번호를 모두 리스트에 저장하여 predict_class 에 할당합니다.\n",
        "\n",
        "predict_class = [load_and_preprocessing(frog_dir, filename, model) for filename in filenames]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWh86jS_zCMc"
      },
      "source": [
        "- **클래스의 이름을 다운받아 출력하여봅니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vcs6pMZh15Pw"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/anishathalye/imagenet-simple-labels.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6whAaTl17DM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open ('/content/imagenet-simple-labels/imagenet-simple-labels.json') as f:\n",
        "    labels = json.load(f)\n",
        "\n",
        "for i in predict_class:\n",
        "    print(labels[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK0FQ726ull1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pm6in4HlvRW"
      },
      "source": [
        "## 2. U-Net\n",
        "\n",
        "Lecture Note에서는 U-Net의 백본(backbone)모델로 `MobileNetV2`를 사용하여 segmentation을 수행하였습니다.<br/>\n",
        "이번 SC에서는 ResNet50을 백본으로 하여 같은 문제를 풀어보세요.\n",
        "\n",
        "참고로 resnet에서의 block은 아래 예시의 3개의 레이어를 참조하여 만들어주세요.<br/>\n",
        "예시는 16x16 까지만 나타나 있지만 Lecture Note 와 같이 4x4까지 만들어 주어야 모델을 완성할 수 있습니다.\n",
        "\n",
        "```\n",
        "    'conv1_relu', # 64x64\n",
        "    'conv2_block3_out', # 32x32 \n",
        "    'conv3_block4_out', # 16x16\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdRNHavXw_1a"
      },
      "outputs": [],
      "source": [
        "# 이 곳에 답안을 작성해 주시기 바랍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "626zYgjkO7Vq"
      },
      "source": [
        "## 3. Free Response\n",
        "> 동료들에게 설명한다고 생각하고 간단하게 몇 문장으로 작성하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__lDWfcUO8oo"
      },
      "source": [
        "### 3.1 딥러닝이 왜 중요하다고 생각하시나요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN3Tx-WfOb6h"
      },
      "source": [
        "`답안 작성은 이 곳에 하시길 바랍니다`\n",
        "\n",
        "인간의 뉴런 구조를 모방하여 만든 딥러닝은, 문제를 학습하는데 탁월한 성능을 보이기 때문이다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSA0mmLdOb6h"
      },
      "source": [
        "### 3.2 딥러닝의 다양한 분야 중에서 좀 더 심도있게 배우고 싶은 분야는 무엇인가요? 왜 그렇게 생각하시나요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ7WTEM1Ob6h"
      },
      "source": [
        "`답안 작성은 이 곳에 하시길 바랍니다`\n",
        "\n",
        "NLP 분야를 깊게 공부해보고 싶다. 산업에서 가장 많은 부분을 차지하는 것이 언어자료라고 생각한다. 방대한 언어자료를 기계가 학습할 수 있다면 큰 가치를 낼 수 있을 것이라 생각한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6tTlgXJOb6h"
      },
      "source": [
        "### 3.3 인공지능이 우리 사회에서 대체할 수 있는 직업이 무엇이라고 생각하시나요?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoP4TdsUOb6h"
      },
      "source": [
        "`답안 작성은 이 곳에 하시길 바랍니다`\n",
        "\n",
        "단순 반복 업무가 적용되는 모든 직업에서 변화가 생겨날 것이라 생각한다. 또한 반복 업무 뿐만 아니라 최근에는 인간의 영역이라 생각했던 창의적으로 생성해내는 영역까지 딥러닝이 영향을 끼칠 것 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jiZ6a7OOb6h"
      },
      "source": [
        "### 3.4 반대로 인공지능 때문에, 딥러닝 때문에 더 생겨날 직업은 무엇이 있을까요?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR918IulOb6h"
      },
      "source": [
        "`답안 작성은 이 곳에 하시길 바랍니다`\n",
        "\n",
        "딥러닝을 활용하는 직업이 생겨날 것 같다. 예를들어 비전을 이용한 VR 산업이 활발해지고 있는데, VR로 컨텐츠나 2차 창작물을 만들어내는 직업이 생길 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruR370r3Ob6h"
      },
      "source": [
        "### 3.5 여러분이 생각하실 때 Strong AI라고 불리우는 [일반 인공지능 (Artificial General Intelligence)](https://ko.wikipedia.org/wiki/%EC%9D%B8%EA%B3%B5_%EC%9D%BC%EB%B0%98_%EC%A7%80%EB%8A%A5)을 개발해내는 것이 가능할 것 같나요? 왜 그렇게 생각하시나요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaybQ4huOb6h"
      },
      "source": [
        "`답안 작성은 이 곳에 하시길 바랍니다`\n",
        "\n",
        "'상상하는 모든 것은 현실이 된다.'는 피카소의 말처럼 Strong AI를 목표로 두고 있다면 언젠가 실현될 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nGc8_Zavpvm"
      },
      "source": [
        "### 3.6 Coutinous learning에 대해서 조사해보고 인지한 부분에 대해서 작성해보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNVkdAP6vw7K"
      },
      "source": [
        "`답안 작성은 이 곳에 하시길 바랍니다`\n",
        "\n",
        "인간의 뇌는 배경지식을 바탕으로 새로운 것을 배우며, 새로운 것을 배운다고 하여 과거의 지식을 잊어버리지 않는다.\n",
        "\n",
        "딥러닝에도 이를 적용하여 다중과제학습이나 온라인 학습을 적용하는 것을 의미한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hoqe3mM_Mtc"
      },
      "source": [
        "# Advanced Goals: 3점을 받기 위해서는 아래의 조건 중에서 3개 이상을 달성하셔야 합니다\n",
        "\n",
        "### 1\n",
        "    - 개구리 이외의 다른 객체를 탐지하는 모델을 만들어보세요 (예: 물고기)\n",
        "    - 이미지를 예측한 label과 같이 출력해보세요\n",
        "    - 예측 모델을 함수로 만들어 보세요 (물론 주석도 잘 되어있어야 합니다)\n",
        "### 2\n",
        "    - U-Net 을 직접 구현하여 동일한 문제를 수행해보세요. \n",
        "### 3\n",
        "    - 작성한 답안을 몇 문장보다 조금 더 상세하게 작성해보세요\n",
        "    - 왜 그렇게 생각하게 되었는 지 관련 근거를 서술하세요.\n",
        "### 추가과제\n",
        "    - GAN을 이용한 프로젝트를 새롭게 구현해보세요."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ai08-sc43x-정석희.ipynb",
      "provenance": []
    },
    "kernel_info": {
      "name": "u4-s3-dnn"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "nteract": {
      "version": "0.23.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}